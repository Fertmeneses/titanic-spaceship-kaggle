{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Title\"></a>\n",
    "# <span style=\"color:teal;font-weight:bold;\">Spaceship Titanic üí° Feature engineering</span>\n",
    "\n",
    "This notebook is the second part of my <span style=\"font-weight:bold;color:green\">Spaceship Titanic series</span>:\n",
    "\n",
    "1. <a href=\"https://www.kaggle.com/code/fertmeneses/spaceship-titanic-getting-familiar/edit/run/191353629\">Spaceship Titanic üèÅ Getting familiar</a>.\n",
    "2. <span style=\"font-weight:bold\">Spaceship Titanic üí° Feature engineering.</span> [This notebook]\n",
    "3. Spaceship Titanic ü©π Data imputation. (Coming soon)\n",
    "4. Spaceship Titanic üñ•Ô∏è Model optimization. (Coming soon)\n",
    "5. Spaceship Titanic üî≠ Integrated analysis. (Coming soon)\n",
    "\n",
    "Previously, I got a clear picture of the scoring expectation for this competition by studying the LeaderBoard and trying simple Machine Learning models with minimally edited data. From that experience, I learned that scoring above 0.80 are pretty good, but <span style=\"font-weight:bold;\">my aim is getting a score above 0.81, looking for a place among the top 5% submissions</span>.\n",
    "\n",
    "<span style=\"font-weight:bold;\">In this notebook, I focus on feature engineering and see the scoring improvement using simple models.</span> \n",
    "\n",
    "In the future notebooks, I'll fix the missing values with data imputation techniques, optimize the Machine Learning model and finally make an integrated analysis based on my results and a deep study of other kagglers' contributions.\n",
    "\n",
    "The methodology of this work is based on my previous notebook <a href=\"https://www.kaggle.com/code/fertmeneses/titanic-kaggle-full-analysis\">Titanic/Kaggle -Full analysis</a> from the <span style=\"font-style:italic;\">Titanic - Machine Learning from Disaster</span> competition.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='teal'>Outline</font> <a class=\"anchor\"  id=\"Outline\"></a>\n",
    "\n",
    "[**Data analysis**](#Data_analysis)\n",
    "\n",
    "  - [Load original data](#Data_analysis_load)\n",
    "  \n",
    "  - [Distribution of values and transported rates](#Data_analysis_values_rates)\n",
    "  \n",
    "  - [Correlations I (original data)](#Data_analysis_correlations)\n",
    "  \n",
    "[**Feature engineering**](#Feature_engineering)\n",
    "\n",
    "  - [Original single features](#Feature_engineering_single)\n",
    "\n",
    "    - [\"PassengerId\": new feature \"GroupMembers\"](#DA_FE_GroupMembers)\n",
    "   \n",
    "    - [\"CryoSleep\": make boolean](#DA_FE_CryoSleep)\n",
    "   \n",
    "    - [\"Cabin\": new features \"Cabin_1st\" and \"Cabin_isP\"](#DA_FE_Cabin)\n",
    "    \n",
    "    - [\"Destination\" redefinition](#DA_FE_Destination)\n",
    "   \n",
    "    - [\"VIP\": make boolean](#DA_FE_VIP)\n",
    "   \n",
    "    - [Expense features: new features \"X_Range\"](#DA_FE_Expense_Range)\n",
    "   \n",
    "    - [\"Name\" feature: new feature \"Ocurrence_LastName\"](#DA_FE_Name)\n",
    "   \n",
    "    - [Correlations II (single engineered features)](#Feature_engineering_single_corr)\n",
    "   \n",
    "  - [Combined features](#Feature_engineering_combined)\n",
    "\n",
    "    - [\"FromTo\" new feature (from \"HomePlanet\" and \"Destination\")](#DA_FE_FromTo)\n",
    "   \n",
    "[**Submission results**](#Submission_results)\n",
    "  \n",
    "[**Conclusions**](#Conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data_analysis\"></a>\n",
    "# <span style=\"color:teal;font-weight:bold;\">Data analysis</span>\n",
    "\n",
    "In this section, I load the original data and analyze it without any edition. This is the workflow:\n",
    "\n",
    "- Load raw data.\n",
    "\n",
    "- Analyze distribution of values and <span style=\"font-weight:bold;\">Transported</span> rates.\n",
    "\n",
    "- Analyze correlations. <span style=\"font-style:italic;\">Note: for this step, I make a copy of the original dataset and One-Hot encode some features, but I don't use this copy for anything else.</span>\n",
    "\n",
    "<a id=\"Data_analysis_load\"></a>\n",
    "## <span style=\"color:teal;font-weight:bold;\">Load original data</span>\n",
    "\n",
    "In the following lines, I load the original datasets and get this information:\n",
    "\n",
    "- Example for 5 first rows in training dataset.\n",
    "\n",
    "- Number of rows in both datasets.\n",
    "\n",
    "- Features' names and data types.\n",
    "\n",
    "- Number of missing values in both datasets, per feature and per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "# Load original datasets:\n",
    "train_df = pd.read_csv('kaggle/input/spaceship-titanic/train.csv') # Training dataset\n",
    "test_df = pd.read_csv('kaggle/input/spaceship-titanic/test.csv') # Testing dataset\n",
    "# Keep the passengerID features separately:\n",
    "train_ID = train_df['PassengerId']\n",
    "test_ID = test_df['PassengerId']\n",
    "# Display a few examples:\n",
    "display(train_df.head(5)) # Examples\n",
    "# Print global information:\n",
    "print('\\nNumber of rows in train/test datasets:\\n')\n",
    "print(len(train_df),'/',len(test_df))\n",
    "print('\\nFeatures: names and data types:\\n')\n",
    "print(train_df.dtypes)\n",
    "# Print number of missing values per feature:\n",
    "print('\\nMissing values in train/test datasets:\\n')\n",
    "for col in test_df.columns:\n",
    "    # Count missing values:\n",
    "    N_train = train_df[col].isna().sum() \n",
    "    N_test = test_df[col].isna().sum()\n",
    "    # Print results:\n",
    "    color_train = 'red' if N_train else 'green'\n",
    "    color_test = 'red' if N_test else 'green'\n",
    "    rmargin = 40-len(col)\n",
    "    print(f'{col}:',f'{colored(N_train, color_train)}/{colored(N_test, color_test)}'.rjust(rmargin))\n",
    "# Count missing values in each row:\n",
    "N_nan_train = train_df.apply(lambda x: x.isna().sum(), axis=1)\n",
    "N_nan_test = test_df.apply(lambda x: x.isna().sum(), axis=1) \n",
    "# Print number of rows with N missing values:\n",
    "for n in set(N_nan_train).union(set(N_nan_test)):\n",
    "    print(f'Number of rows with {n} missing values: {sum(N_nan_train==n)}/{sum(N_nan_test==n)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data_analysis_values_rates\"></a>\n",
    "## <span style=\"color:teal;font-weight:bold;\">Distribution of values and Transported rates</span>\n",
    "\n",
    "For a better understanding of each feature, I <span style=\"font-weight:bold;\">plot the distribution of values (column 1) in both training and testing datasets; and the Transported rates for the training dataset (column 2)</span>. Those feature with more than 10 unique values are not included in the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Define plotting functions:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_dist_pie(df_train,df_test,feature,axis):\n",
    "    \"\"\"\n",
    "    Plots a pie chart for the distribution of values from a single\n",
    "    feature in both training and testing datasets.\n",
    "    --- Inputs ---\n",
    "    {df_train, df_test} [Dataframes]: Training and testing datasets.\n",
    "    Both datasets must include the column with the input name {feature}.\n",
    "    {feature} [String]: Name of the column (feature) to be analyzed.\n",
    "    {axis} [matplotlib axis object]: Axis for the current subplot.\n",
    "    \"\"\"\n",
    "    # Identify data, count frequency per variable and sort it alphabetically:\n",
    "    train_data, test_data = df_train[feature], df_test[feature]\n",
    "    train_counts = train_data.value_counts().sort_index()\n",
    "    test_counts = test_data.value_counts().sort_index()\n",
    "    # Plot pie chart, start with the outer ring:\n",
    "    axis.pie(train_counts, colors=sns.color_palette('tab20'),\n",
    "             labels=train_counts.keys(),labeldistance=0.8,\n",
    "             startangle=90,frame=True,explode=np.ones(len(train_counts))*0.01)\n",
    "    # Add white ring to separate training and testing pies:\n",
    "    axis.add_artist(plt.Circle((0,0),0.70,color='black', fc='white',linewidth=0))\n",
    "    # Testing inner pie:\n",
    "    axis.pie(test_counts, colors=sns.color_palette('tab20'),\n",
    "             labels=None,labeldistance=0.6,\n",
    "             radius=0.5,startangle=90,explode=np.ones(len(test_counts))*0.01)\n",
    "    # Add white central circle to complete the pie:\n",
    "    axis.add_artist(plt.Circle((0,0),0.25,color='black', fc='white',linewidth=0))\n",
    "    # Set title:\n",
    "    axis.set_title(f'{feature}: Distribution')\n",
    "    \n",
    "def plot_dist_hist(df_train,df_test,feature,axis,bin_step=None):\n",
    "    \"\"\"\n",
    "    Plots a histogram chart for the distribution of values from a\n",
    "    single feature in both training and testing datasets.\n",
    "    --- Inputs ---\n",
    "    {df_train, df_test} [Dataframes]: Training and testing datasets.\n",
    "    Both datasets must include the column with the input name {feature}.\n",
    "    {feature} [String]: Name of the column (feature) to be analyzed.\n",
    "    {axis} [matplotlib axis object]: Axis for the current subplot.\n",
    "    {bin_step} [Integer or None]: If provided, set the bins' \n",
    "    step-value for the histogram, otherwise automatically assigned.\n",
    "    \"\"\"\n",
    "    # Identify relevant data and calculate fraction of valid values:\n",
    "    train_data, test_data = df_train[feature], df_test[feature]\n",
    "    train_frac = np.round(train_data.count()/len(train_data)*100,1) # [%]\n",
    "    test_frac = np.round(test_data.count()/len(test_data)*100,1) # [%]\n",
    "    # Determine binning, uniform for both datasets:\n",
    "    min_range = min(min(train_data),min(test_data))\n",
    "    max_range = max(max(train_data),max(test_data))\n",
    "    if bin_step:\n",
    "        binning = np.arange(min_range,max_range+bin_step*2,bin_step)-bin_step/2\n",
    "    else:\n",
    "        binning = np.linspace(min_range,max_range,10)\n",
    "    # Bar plot:\n",
    "    train_data.plot(kind='hist', bins=binning, edgecolor='navy', color='teal',\n",
    "                    ax=axis, alpha=0.6)\n",
    "    test_data.plot(kind='hist', bins=binning, edgecolor='navy', color='orange',\n",
    "                   ax=axis, alpha=0.6, rwidth=0.7)\n",
    "    axis.legend(['Train','Test']) # Set legend   \n",
    "    axis.set_yticks([0,int(axis.get_ylim()[1])], minor=False)\n",
    "    axis.set_xlabel(feature)\n",
    "    axis.set_title(f'{feature}: Distribution')\n",
    "    for s in [\"top\",\"right\",\"left\", 'bottom']: # Remove spins\n",
    "        axis.spines[s].set_visible(False)\n",
    "\n",
    "def plot_swarm(df_train,feature,axis,ref_feature='Transported',\n",
    "               xlabels_off=False,seed=42):\n",
    "    \"\"\"\n",
    "    Plots a swarm plot ordered by the transported rate from a \n",
    "    single feature in the training dataset.\n",
    "    --- Inputs ---\n",
    "    {df_train} [Dataframe]: Training dataset, must include the\n",
    "    columns with names {feature} and {ref_feature}.\n",
    "    {feature} [String]: Name of the column (feature) to be analyzed.\n",
    "    {ref_feature} [String]: Name of the column (feature) by which {feature} will\n",
    "    be analyzed. This feature must be binary with values 0 and 1.\n",
    "    {axis} [matplotlib axis object]: Axis for the current subplot.\n",
    "    {xlabels_off} [Boolean]: If True, remove the x-labels.\n",
    "    {seed} [Integer]: Seed for random scattering in swarm plots.\n",
    "    \"\"\"\n",
    "    # Identify data, count frequency per variable and sort it alphabetically:\n",
    "    train_data = df_train[feature] # Training dataset    \n",
    "    train_counts = train_data.value_counts().sort_index()\n",
    "    # For each variable, identify the transported rate and build the swarm plot:\n",
    "    np.random.seed(seed) # Random seed for swarm plots\n",
    "    for i, var in enumerate(train_counts.keys()):\n",
    "        # Determine points' location:\n",
    "        surv_rate = df_train.groupby([feature]).mean(numeric_only=True)[ref_feature].loc[var] # Transported rate\n",
    "        pp_pos = int(len(df_train[(df_train[feature]==var)])*surv_rate) # Positive transported values\n",
    "        pp_neg = int(len(df_train[(df_train[feature]==var)])*(1-surv_rate)) # Negative transported values\n",
    "        var_pos = np.random.uniform(0, surv_rate,pp_pos) # Randomly assign locations for \"positive\" points\n",
    "        var_neg = np.random.uniform(surv_rate,1,pp_neg) # Randomly assign locations for \"negative\" points              \n",
    "        # Allocate all points in plot:\n",
    "        color = np.array(sns.color_palette('tab20')[i])\n",
    "        axis.scatter(i+np.random.uniform(-0.3, 0.3, len(var_neg)), var_neg,s=10,\n",
    "                     color=color, edgecolor=np.append(color,0.2),\n",
    "                     alpha=0.1, label=f'{var}({ref_feature}=0)')\n",
    "        axis.scatter(i+np.random.uniform(-0.3, 0.3, len(var_pos)), var_pos,s=10,\n",
    "                     color=color, edgecolor=np.append(color,0.2),\n",
    "                     label=f'{var}({ref_feature}=1)')\n",
    "        axis.plot([i-0.3,i+0.3],[surv_rate,surv_rate],ls='--',color='k',lw=1)\n",
    "    # Ticks and limits:\n",
    "    axis.set_xlim(-0.5, len(train_counts)-0.5)\n",
    "    axis.set_ylim(-0.03, 1.1)\n",
    "    if xlabels_off:\n",
    "        axis.set_xticks([])\n",
    "        axis.set_xticklabels([])\n",
    "    else:\n",
    "        axis.set_xticks(np.linspace(0,len(train_counts.keys())-1,len(train_counts.keys())))\n",
    "        axis.set_xticklabels(train_counts.keys(), fontsize=10)\n",
    "    axis.set_yticks([], minor=False)\n",
    "    axis.set_ylabel('')\n",
    "    # Spines, legend and title:\n",
    "    for s in [\"top\",\"right\",\"left\", 'bottom']:\n",
    "        axis.spines[s].set_visible(False)\n",
    "    axis.legend([0,1],title=ref_feature,loc=(0.8, 0.7), edgecolor='k')\n",
    "    axis.set_title(f'{feature}: \"{ref_feature}\" rate (Train)')\n",
    "\n",
    "import warnings # <sns.kdeplot> gives a warning I couldn't fix, I avoid displaying it...\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def plot_KDE(df_train,feature,axis,ref_feature='Transported'):\n",
    "    \"\"\"\n",
    "    Plots a kernel density estimate (KDE) plot ordered by the \n",
    "    transported rate from a single feature in the training dataset.\n",
    "    --- Inputs ---\n",
    "    {df_train} [Dataframe]: Training dataset, must include the\n",
    "    columns with names {feature} and {ref_feature}.\n",
    "    {feature} [String]: Name of the column (feature) to be analyzed.\n",
    "    {ref_feature} [String]: Name of the column (feature) by which {feature} will\n",
    "    be analyzed. This feature must be binary with values 0 and 1.\n",
    "    {axis} [matplotlib axis object]: Axis for the current subplot.\n",
    "    \"\"\"    \n",
    "    train_data = df_train[feature] # Identify relevant data\n",
    "    # Plot KDE:\n",
    "    sns.kdeplot(x=feature, data=df_train, ax=axis, fill=True,cut=0,\n",
    "                bw_method=0.15, lw=1.4, edgecolor='lightgray', hue=ref_feature,\n",
    "                multiple=\"stack\", palette='PuBu', alpha=0.8)\n",
    "    axis.set_yticks([], minor=False)\n",
    "    axis.set_ylabel('Density [arb. units]')\n",
    "    axis.set_title(f'{feature}: \"{ref_feature}\" rate (Train)')\n",
    "    for s in [\"top\",\"right\",\"left\", 'bottom']: # Remove spins\n",
    "        axis.spines[s].set_visible(False)  \n",
    "\n",
    "def explain_stats(df_train,feature,unique_vals_max=10):\n",
    "    \"\"\"\n",
    "    Explains the distribution of values in a table format and the\n",
    "    transported rates for the training dataset.\n",
    "    --- Inputs ---\n",
    "    {df_train} [Dataframe]: Training dataset, must include the\n",
    "    columns with names {feature} and {ref_feature}.\n",
    "    {feature} [String] Name of the column (feature) to be analyzed.\n",
    "    {unique_vals_max} [Integer]: Maximum number of unique values \n",
    "    for the feature. If there are more than this limit, there \n",
    "    won't be any output table.\n",
    "    \"\"\"\n",
    "    # Check the unique_vals_max condition (avoid NaN):\n",
    "    unique_vals = {x for x in df_train[feature] if x == x} # Number of unique values\n",
    "    if len(unique_vals) > unique_vals_max:\n",
    "        print(f'There are {len(unique_vals)} unique values for this feature, more than the allowed limit ({unique_vals_max}).')\n",
    "        return None    \n",
    "    # Explained values:\n",
    "    df_expl = pd.DataFrame(columns=[f'{feature}','#Passengers','Transported_Rate[%]'])\n",
    "    df_expl[f'{feature}'] = sorted(unique_vals)\n",
    "    df_expl['#Passengers'] = [len(df_train[df_train[f'{feature}'] == x])\n",
    "                              for x in sorted(unique_vals)]\n",
    "    # Transported rates for each feature category:\n",
    "    df_expl['Transported_Rate[%]'] = [\n",
    "        len(df_train[(df_train[feature] == x) & # Select fare range\n",
    "            (df_train['Transported'] == 1)])/ # Passengers who were transported\n",
    "        len(df_train[(df_train[feature] == x)]) # Total passengers\n",
    "        for x in sorted(unique_vals) # Iterate through all feature categories\n",
    "    ]\n",
    "    df_expl['Transported_Rate[%]'] *=100 # Convert from fraction to [%]\n",
    "    print(f'Explained {feature} values and transported rates in training dataset:')\n",
    "    display(df_expl.style.hide())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features to analyze:\n",
    "features = [feat for feat in test_df.columns if \n",
    "            test_df[feat].dtype != 'object' or # Numeric or...\n",
    "            test_df[feat].nunique()<10] # ...Few unique values\n",
    "\n",
    "# Plot distribution of values and Transported rates for each feature:\n",
    "for feature in features:\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2,figsize=(8, 3)) # Start figure\n",
    "    if train_df[feature].dtype == 'object': # Non-numerical features:\n",
    "        print('-'*10,f'{feature} | Outer:train, Inner:test','-'*10)\n",
    "        plot_dist_pie(train_df,test_df,feature,ax1)\n",
    "        plot_swarm(train_df,feature,ax2)    \n",
    "    else: # Numerical features:\n",
    "        print('-'*25,feature,'-'*25)\n",
    "        plot_dist_hist(train_df,test_df,feature,ax1)\n",
    "        plot_KDE(train_df,feature,ax2)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    # Display explanatory table only if there are equal or less than 10 unique values:\n",
    "    if len(set(train_df[feature]))<=10:\n",
    "        explain_stats(train_df,feature)\n",
    "    else:\n",
    "        print('(Not suitable for a table)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary about the distribution of values and transported rates:\n",
    "\n",
    "| Feature | <font color=blue> Distribution </font> | <font color=green> Transported rates </font> |\n",
    "| :---: | :--- | :--- |\n",
    "| <span style=\"font-weight:bold;\">HomePlanet</span> | <font color=blue> Half of the passengers come from Earth, 1/4 from Europa and 1/4 from Mars  </font> | <font color=green> Europa has a slightly good rate (65%), Mars average (50%) and Earth slightly poor (40%) </font> |\n",
    "| <span style=\"font-weight:bold;\">CryoSleep</span> | <font color=blue> Only 1/3 of passengers were in CryoSleep  </font> | <font color=green> Excellent chances (80%) if Cryosleep, poor (30%) if not </font> |\n",
    "| <span style=\"font-weight:bold;\">Destination</span> | <font color=blue> 2/3 of passengers were going to Trappist, 1/4 to Cancri and 10% to PSO </font> | <font color=green> Those going to Cancri have good chances (60%), the rest average (50%) </font> |\n",
    "| <span style=\"font-weight:bold;\">Age</span> | <font color=blue> From 0 to 80, peak around 20-30 </font> | <font color=green> Good chances for the very young (less than 10 years old?), maybe average for the rest </font> |\n",
    "| <span style=\"font-weight:bold;\">VIP</span> | <font color=blue> Only 200 passengers are VIP </font> | <font color=green> VIP passengers have slightly bad chances (40%) </font> |\n",
    "| <span style=\"font-weight:bold;\">RoomService, FoodCourt, ShoppingMall, Spa, VRDeck</span> | <font color=blue> Only a few passengers spend more than a little money in services </font> | <font color=green> Can't tell from the plots </font> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data_analysis_correlations\"></a>\n",
    "## <span style=\"color:teal;font-weight:bold;\">Correlations I (original data)</span>\n",
    "\n",
    "Next, I study the <span style=\"font-weight:bold;\">correlations in the original data, using the training dataset, only for those features analyzed in the previous section</span>. I build a correlation matrix with a 0.1 threshold correlation value, then only high correlations are painted.\n",
    "\n",
    "Note: the correlation matrix needs numerical or boolean features. For this purpose, I One-Hot encode the non-numerical features, and group family-encoded-features in dotted traingles within the correlation matrix (correlations there are irrelevant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "def encode_and_correlation(df,max_cats=10,target_col='Transported',\n",
    "                           c_thres=0.1,plot_corr=True):\n",
    "    \"\"\"\n",
    "    Encodes non-numeric features in a dataset and plots the correlation matrix.\n",
    "    --- Inputs ---\n",
    "    {df} [dataframe]: Dataframe to be encoded.\n",
    "    {max_cats} [int]: Maximum number of categories for a non-numeric\n",
    "    feature to be encoded. If the feature has more categories, it is ignored.\n",
    "    {ref_col} [string]: Target column, it won't be encoded.\n",
    "    {c_thres} [float]: Threshold correlation value, only exceeding values\n",
    "    will be painted in the correlation matrix.\n",
    "    {plot_corr} [boolean]: If True, plots the correlation matrix. If False, \n",
    "    doesn't plot it, but still returns the encoded dataframe.\n",
    "    --- Outputs ---\n",
    "    {df_enc} [dataframe]: Encoded dataframe.\n",
    "    \"\"\"\n",
    "    # ===== ENCODING ===== \n",
    "    # One-Hot encode non-numerical features with few unique values:\n",
    "    feat_enc = [feat for feat in df.columns if \n",
    "                df[feat].dtype == 'object' and # Numeric or...\n",
    "                df[feat].nunique()<=max_cats] # ...Few unique values\n",
    "\n",
    "    # Encode features:\n",
    "    data_enc = pd.get_dummies(df[feat_enc])\n",
    "    # Prepare dataframes having only numeric/boolean features:\n",
    "    df_enc = pd.concat([df, data_enc], axis=1)\n",
    "    # Sort columns alphabetically, but leave 'Transported' at the beggining in training dataset:\n",
    "    df_enc = df_enc.reindex(sorted(df_enc.columns), axis=1)\n",
    "    if target_col in df_enc:\n",
    "        df_enc = df_enc[['Transported'] + [col for col in df_enc.columns\n",
    "                                                       if col != target_col ]]\n",
    "    else:\n",
    "        df_enc = df_enc[[col for col in df_enc.columns]]\n",
    "    # Drop source-encoded features: \n",
    "    df_enc = df_enc.drop(feat_enc,axis=1)\n",
    "    # Drop not suitable features:\n",
    "    feat_not_enc = [feat for feat in df.columns if\n",
    "                    df[feat].dtype == 'object' and # Numeric or...\n",
    "                    df[feat].nunique()>10] # ...Few unique values\n",
    "    df_enc = df_enc.drop(feat_not_enc,axis=1)\n",
    "    \n",
    "    # ===== CORRELATION MATRIX ===== \n",
    "    if plot_corr:\n",
    "        # Prepare data:\n",
    "        corr = df_enc.corr() # Obtain correlations\n",
    "        trimask = np.triu(np.ones_like(corr, dtype=bool)) # Mask upper triangle in correlations    \n",
    "        # Plot correlations in training dataset:\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        sns.heatmap(corr, ax=ax, square=True,cmap='coolwarm',\n",
    "                    vmin=-1, vmax=1, linecolor='w',lw=0.5, \n",
    "                    mask=trimask | (np.abs(corr) <= c_thres))\n",
    "        # Separate the target correlations:\n",
    "        ax.plot([1,1],[1,len(df_enc.columns)],color='k',lw=2)\n",
    "        # Draw the correlations' triangle:\n",
    "        ax.plot([1,len(df_enc.columns)],[1,len(df_enc.columns)],color='k',lw=0.5)\n",
    "        ax.plot([1,len(df_enc.columns)],[len(df_enc.columns),len(df_enc.columns)],\n",
    "                color='k',lw=1.5)\n",
    "        # Identify blocks of similar features:\n",
    "        main_name = [feat.split('_')[0] for feat in list(df_enc.columns)]\n",
    "        index_counts = [(main_name.index(name),main_name.count(name))\n",
    "                        for name in sorted(set(main_name))]\n",
    "        for index, counts in index_counts:\n",
    "            if counts>1: \n",
    "                triangle = [[index, index],\n",
    "                            [index+counts, index+counts],\n",
    "                            [index, index+counts]]\n",
    "                ax.add_patch(patches.Polygon(triangle, edgecolor=\"k\",facecolor='none',ls=\"--\",lw=2))\n",
    "        ax.set_title(f\"Dataset Correlations\", size=15)\n",
    "        fig.tight_layout()\n",
    "        plt.show()    \n",
    "    \n",
    "    return df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_enc = encode_and_correlation(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T08:54:31.420188Z",
     "iopub.status.busy": "2024-08-07T08:54:31.419691Z",
     "iopub.status.idle": "2024-08-07T08:54:32.254539Z",
     "shell.execute_reply": "2024-08-07T08:54:32.253400Z",
     "shell.execute_reply.started": "2024-08-07T08:54:31.420151Z"
    }
   },
   "source": [
    "I'm going to do a more detailed analysis later, but from this plot I highlight the following facts:\n",
    "\n",
    "<span style=\"font-weight:bold;\">Chances of transportation (first column):</span> \n",
    "- <span style=\"font-weight:bold;\">CryoSleep</span>: Cryo-sleeping (True) is better.\n",
    "- <span style=\"font-weight:bold;\">Destination</span>: going to Cancri is better.\n",
    "- <span style=\"font-weight:bold;\">HomePlanet</span>: being from Earth is worse, from Europa is better. \n",
    "- <span style=\"font-weight:bold;\">RoomService</span>: paying more is worse.\n",
    "- <span style=\"font-weight:bold;\">Spa</span>: paying more is worse.\n",
    "- <span style=\"font-weight:bold;\">VRDeck</span>: paying more is worse.\n",
    "\n",
    "<span style=\"font-style:italic;\">Note: the correlation matrix is strongly affected by the number of values with significant correlations, that's why the <span style=\"font-weight:bold;\">VIP</span> feature is not painted, even though being VIP is an important indicator for Transportation. </span>\n",
    "\n",
    "<span style=\"font-weight:bold;\">Other observations (main triangle):</span>\n",
    "\n",
    "- Cryo-sleeping passengers spend less in services \n",
    "(<span style=\"font-weight:bold;\">FoodCourt</span>\n",
    "<span style=\"font-weight:bold;\">RoomService</span>, \n",
    "<span style=\"font-weight:bold;\">ShoppingMall</span>,\n",
    "<span style=\"font-weight:bold;\">Spa</span>,\n",
    "<span style=\"font-weight:bold;\">VRDeck</span>).\n",
    "- The older the passenger, the more more likely is that they are from Europa. On the contrary, younger passengers are more likely to come from Earth.\n",
    "- The following origin-destinations are more frequent: Europa-Cancri, Earth-PSO, Mars-Trappist.\n",
    "- Passengers from Mars are more likely to spend more in RoomService and ShoppingMall.\n",
    "- Passengers from Europa are more likely to spend more in Spa and VRDeck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Feature_engineering\"></a>\n",
    "# <span style=\"color:teal;font-weight:bold;\">Feature engineering</span>\n",
    "\n",
    "In this section, I first engineer single original features, extracting encoded information from then, and later I generate new variables based on combination of two or more features.\n",
    "\n",
    "By the end of the data engineering process, each feature in the dataset should be numeric, boolean, or non-numeric with a few unique values (so they can be easily encoded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First generate the corrected datasets:\n",
    "train_df_corr = train_df.copy()\n",
    "test_df_corr = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Feature_engineering_single\"></a>\n",
    "## <span style=\"color:teal;font-weight:bold;\">Original single features</span>\n",
    "\n",
    "The original features can contain encoded information that may be stored in new variables, or too complex information that could be simplified, or completely irrelevant information that could be deleted.\n",
    "\n",
    "I analyze the original features in the order given in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Order in original features:\\n',[f for f in test_df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"DA_FE_GroupMembers\"></a>\n",
    "### <span style=\"color:teal;font-weight:bold;\">\"PassengerId\": new feature \"GroupMembers\"</span>\n",
    "\n",
    "I begin by studying the <span style=\"font-weight:bold;\">PassengerId</span> feature. According to the official definition:\n",
    "\n",
    "<span style=\"font-weight:bold;\">PassengerId</span> - A unique Id for each passenger. <span style=\"font-weight:bold;\">.Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group</span>. People in a group are often family members, but not always.\n",
    "\n",
    "I split <span style=\"font-weight:bold;\">PassengerId</span> into the new features <span style=\"font-weight:bold;\">IDgroup</span> (gggg) and <span style=\"font-weight:bold;\">IDnumber</span> (pp) and study their unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset:\n",
    "train_df_corr['IDgroup'] = train_df['PassengerId'].apply(\n",
    "    lambda x: int(x.split('_')[0]))\n",
    "train_df_corr['IDnumber'] = train_df['PassengerId'].apply(\n",
    "    lambda x: int(x.split('_')[1]))\n",
    "# Testing dataset:\n",
    "test_df_corr['IDgroup'] = test_df['PassengerId'].apply(\n",
    "    lambda x: int(x.split('_')[0]))\n",
    "test_df_corr['IDnumber'] = test_df['PassengerId'].apply(\n",
    "    lambda x: int(x.split('_')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique values in IDgroup (training dataset):', len(set(train_df_corr['IDgroup'])))\n",
    "print('Number of unique values in IDnumber (training dataset):', len(set(train_df_corr['IDnumber'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are too many groups, <span style=\"font-weight:bold;\">IDgroup</span> won't be useful for a Machine Learning model... However, identifying member's quantity in each group may help... The <span style=\"font-weight:bold;\">IDnumber</span> gives an idea of the minimum number of people within a group, but the whole picture is missing.\n",
    "\n",
    "I build the new feature <span style=\"font-weight:bold;\">GroupMembers</span>, which identifies for each passenger the number of people sharing their <span style=\"font-weight:bold;\">IDgroup</span>. In order to do that, I consider both training and testing datasets, with the following protocol:\n",
    "\n",
    "* Identify the ocurrences for each unique value in <span style=\"font-weight:bold;\">IDgroup</span>, in the total training+testing dataset.\n",
    "  \n",
    "* Assign the <span style=\"font-weight:bold;\">GroupMembers</span> in each training and testing dataset.\n",
    "\n",
    "* Drop features <span style=\"font-weight:bold;\">IDgroup</span> and <span style=\"font-weight:bold;\">IDnumber</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify ocurrences for every unique value in IDgroup:\n",
    "ocurrences = pd.concat([train_df_corr['IDgroup'], test_df_corr['IDgroup']]).value_counts().to_dict()\n",
    "for dataset in [train_df_corr,test_df_corr]:\n",
    "    dataset['GroupMembers'] = dataset['IDgroup'].apply(lambda x: ocurrences[x])\n",
    "# Drop unnecessary features:\n",
    "for feature in ['PassengerId','IDgroup','IDnumber']:\n",
    "    if feature in train_df_corr: # Just in case feature is already dropped\n",
    "        train_df_corr = train_df_corr.drop(feature,axis=1)\n",
    "        test_df_corr = test_df_corr.drop(feature,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the distribution of values and Transported rates for <span style=\"font-weight:bold;\">GroupMembers</span>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of values and Transported rates:\n",
    "fig, (ax1,ax2) = plt.subplots(1, 2,figsize=(8, 3)) # Start figure\n",
    "plot_dist_hist(train_df_corr,test_df_corr,'GroupMembers',ax1,bin_step=1)\n",
    "plot_KDE(train_df_corr,'GroupMembers',ax2)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "# Display explanatory table:\n",
    "explain_stats(train_df_corr,'GroupMembers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"DA_FE_CryoSleep\"></a>\n",
    "### <span style=\"color:teal;font-weight:bold;\">\"CryoSleep\": make boolean/numeric</span>\n",
    "\n",
    "<span style=\"font-weight:bold;\">CryoSleep</span> feature is very simple, with only True or False values. To make life easier for Machine Learning models, I will convert True to 1 and False to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [train_df_corr,test_df_corr]:\n",
    "    dataset['CryoSleep'] = dataset['CryoSleep'].apply(\n",
    "        lambda x: np.nan if x!=x else (1 if x else 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"DA_FE_Cabin\"></a>\n",
    "### <span style=\"color:teal;font-weight:bold;\">\"Cabin\": new features \"Cabin_1st\" and \"Cabin_isP\"</span>\n",
    "\n",
    "The <span style=\"font-weight:bold;\">Cabin</span> feature has too many unique values and couldn't be analyzed properly. Let's look a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin = list(train_df['Cabin'].loc[~train_df['Cabin'].isna()].values)+\\\n",
    "        list(test_df['Cabin'].loc[~test_df['Cabin'].isna()].values)\n",
    "print(list(set(cabin))[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All values follow the same patter: 'Letter/Number/Letter'. I will generate 3 new features accounting for them: <span style=\"font-weight:bold;\">Cabin_1st</span>, <span style=\"font-weight:bold;\">Cabin_2nd</span> and <span style=\"font-weight:bold;\">Cabin_3rd</span>, respectively. Then, I will drop the original <span style=\"font-weight:bold;\">Cabin</span> feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin_X = ['Cabin_1st','Cabin_2nd','Cabin_3rd']\n",
    "for i,cabin in enumerate(cabin_X):\n",
    "    if cabin not in train_df_corr: # In case faeture it's already created\n",
    "        # Training dataset:\n",
    "        train_df_corr[cabin] = train_df['Cabin'].apply(\n",
    "            lambda x: np.nan if x!=x else (\n",
    "                x.split('/')[i]))\n",
    "        # Testing dataset:\n",
    "        test_df_corr[cabin] = test_df['Cabin'].apply(\n",
    "            lambda x: np.nan if x!=x else (\n",
    "                x.split('/')[i]))\n",
    "# Summarize:\n",
    "for c in cabin_X:\n",
    "    print(f'Unique values in {c} (training/testing):',\n",
    "          len({x for x in train_df_corr[c] if x==x}),'/',\n",
    "          len({x for x in test_df_corr[c] if x==x}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only <span style=\"font-weight:bold;\">Cabin_1st</span> and <span style=\"font-weight:bold;\">Cabin_3rd</span> are suitable for ML models, while <span style=\"font-weight:bold;\">Cabin_2nd</span> has too many unique values.\n",
    "\n",
    "I drop <span style=\"font-weight:bold;\">Cabin_2nd</span> and <span style=\"font-weight:bold;\">Cabin</span>, and then plot the distribution of values and Transported rates for the new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary features:\n",
    "for feature in ['Cabin','Cabin_2nd']:\n",
    "    if feature in train_df_corr: # Just in case feature is already dropped\n",
    "        train_df_corr = train_df_corr.drop(feature,axis=1)\n",
    "        test_df_corr = test_df_corr.drop(feature,axis=1)\n",
    "# Plot distribution of values and Transported rates:\n",
    "for feature in ['Cabin_1st','Cabin_3rd']:\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2,figsize=(8, 3)) # Start figure\n",
    "    print('-'*10,f'{feature} | Outer:train, Inner:test','-'*10)\n",
    "    plot_dist_pie(train_df_corr,test_df_corr,feature,ax1)\n",
    "    plot_swarm(train_df_corr,feature,ax2)    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    # Display explanatory table:\n",
    "    explain_stats(train_df_corr,feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent news! The old feature <span style=\"font-weight:bold;\">Cabin</span> encoded much information that was recovered in the new <span style=\"font-weight:bold;\">Cabin_1st</span> and <span style=\"font-weight:bold;\">Cabin_3rd</span> features.\n",
    "\n",
    "Lastly, because <span style=\"font-weight:bold;\">Cabin_3rd</span> is a binary feature (either 'S' or 'P'), I will redefine it as <span style=\"font-weight:bold;\">Cabin_isP</span>, with values equal to 1 (='P') or 0 (='S')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'Cabin_isP':\n",
    "if 'Cabin_isP' not in train_df_corr: # In case faeture it's already created\n",
    "    # Training dataset:\n",
    "    train_df_corr['Cabin_isP'] = train_df_corr['Cabin_3rd'].apply(\n",
    "        lambda x: np.nan if x!=x else (1 if x=='P' else 0))\n",
    "    # Testing dataset:\n",
    "    test_df_corr['Cabin_isP'] = test_df_corr['Cabin_3rd'].apply(\n",
    "        lambda x: np.nan if x!=x else (1 if x=='P' else 0))\n",
    "# Drop 'Cabin_3rd':\n",
    "if 'Cabin_3rd' in train_df_corr: # Just in case feature is already dropped\n",
    "    train_df_corr = train_df_corr.drop('Cabin_3rd',axis=1)\n",
    "    test_df_corr = test_df_corr.drop('Cabin_3rd',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"DA_FE_Destination\"></a>\n",
    "### <span style=\"color:teal;font-weight:bold;\">\"Destination\" redefinition</span>\n",
    "\n",
    "The current <span style=\"font-weight:bold;\">Destination</span> feature's unique values are hard to read: '55 Cancri e', 'PSO J318.5-22' and 'TRAPPIST-1e'. I redefine them as 'Cancri', 'PSO' and \"Trappist', respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_corr[\"Destination\"] = train_df[\"Destination\"].apply(\n",
    "    lambda x: np.nan if x!=x else (\n",
    "    \"Cancri\" if x==\"55 Cancri e\" else (\n",
    "        \"PSO\" if x==\"PSO J318.5-22\" else \"Trappist\"))\n",
    ")\n",
    "test_df_corr[\"Destination\"] = test_df[\"Destination\"].apply(\n",
    "    lambda x: np.nan if x!=x else (\n",
    "    \"Cancri\" if x==\"55 Cancri e\" else (\n",
    "        \"PSO\" if x==\"PSO J318.5-22\" else \"Trappist\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"DA_FE_VIP\"></a>\n",
    "### <span style=\"color:teal;font-weight:bold;\">\"VIP\": make boolean/numeric</span>\n",
    "\n",
    "I will apply to <span style=\"font-weight:bold;\">VIP</span> the same procedure as <span style=\"font-weight:bold;\">CryoSleep</span>: convert True to 1 and False to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [train_df_corr,test_df_corr]:\n",
    "    dataset['VIP'] = dataset['VIP'].apply(\n",
    "        lambda x: np.nan if x!=x else (1 if x else 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"DA_FE_Expense_Range\"></a>\n",
    "### <span style=\"color:teal;font-weight:bold;\">Expense features: new features \"X_Range\"</span>\n",
    "\n",
    "Features <span style=\"font-weight:bold;\">RoomService</span>,\n",
    "<span style=\"font-weight:bold;\">FoodCourt</span>,\n",
    "<span style=\"font-weight:bold;\">ShoppingMall</span>,\n",
    "<span style=\"font-weight:bold;\">Spa</span> and\n",
    "<span style=\"font-weight:bold;\">VRDeck</span> describe expenses incurred by passengers. \n",
    "\n",
    "They are numerical features that span a wide range (from 0 to thousands) with a very skewed distribution: the vast majority of passengers spend very little, while the rest of the expense distribution is occupied by relatively few passengers. \n",
    "\n",
    "Consequently, I <span style=\"font-weight:bold;\">simplify the information by using a logarithmic distribution</span>.\n",
    "\n",
    "<span style=\"font-style:italic;\">Note: the logarithm operation requires values strictly greater than 0. For this purpose, I consider all 0 values as a tenth fraction the minimum non-zero value. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_feats = ['RoomService','FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "for expense in exp_feats:    \n",
    "    new_feat = f'{expense}_Range' # New engineered feature\n",
    "    # Find minimum fare above 0 and maximum fare:\n",
    "    all_exp = set(pd.concat([train_df[expense], test_df[expense]])) # All fares\n",
    "    min_exp = sorted(all_exp)[1] # Minimum non-zero expense\n",
    "    max_exp = max(all_exp) # Manimum expense\n",
    "    # Create the new Fare_Range feature:\n",
    "    for dataset in [train_df_corr,test_df_corr]:\n",
    "        # First copy the Fare future and fix all zero values:\n",
    "        dataset[new_feat] = dataset[expense]\n",
    "        dataset.loc[dataset[new_feat]==0,new_feat] = min_exp\n",
    "        # Convert to logarithmic scale:\n",
    "        dataset[new_feat] = np.log10(dataset[new_feat])\n",
    "        # Encode to bins with width equal to 1:\n",
    "        dataset[new_feat] = dataset[new_feat].apply(\n",
    "            lambda x: np.nan if x!=x else np.ceil(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in exp_feats:\n",
    "    feat_range = feat+'_Range' # Engineered feature\n",
    "    # Plot distribution of values and survival rates:\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2,figsize=(8, 3)) \n",
    "    plot_dist_hist(train_df_corr,test_df_corr,feat_range,ax1,bin_step=1)\n",
    "    plot_KDE(train_df_corr,feat_range,ax2)    \n",
    "    plt.show()\n",
    "    # Explained values:\n",
    "    feat_expl = pd.DataFrame(columns=[feat_range,feat,'#Passengers','Transported_Rate[%]'])\n",
    "    feat_expl[feat_range] = np.arange(0,int(np.max(train_df_corr[feat_range]))+1,1)\n",
    "    feat_expl[feat] = [f'[{int(10**(i-1))}, {int(10**i)})' for\n",
    "                       i in feat_expl[feat_range]]\n",
    "    feat_expl['#Passengers'] = [len(train_df_corr[train_df_corr[feat_range] == x])\n",
    "                              for x in feat_expl[feat_range]]\n",
    "    # Transported rates for each Expense_Range category:\n",
    "    feat_expl['Transported_Rate[%]'] = [\n",
    "        len(train_df_corr[(train_df_corr[feat_range] == x) & # Select expense range\n",
    "            (train_df_corr['Transported'] == 1)])/ # Passengers who were transported\n",
    "        len(train_df_corr[(train_df_corr[feat_range] == x)]) # Total passengers\n",
    "        for x in feat_expl[feat_range] # Iterate through all expense range values\n",
    "    ]\n",
    "    feat_expl['Transported_Rate[%]'] *= 100 # Convert from fraction to [%]\n",
    "    print(f'Explained {feat_range} values and Transported rates in training dataset:')\n",
    "    display(feat_expl.style.hide())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information is much simpler now and easier to visualize. There are some general highlights:\n",
    "\n",
    "* For all new ranged-features, the most common category is 0, integrated mainly by passengers with zero expenses. Transported rates in all cases are close to 60%.\n",
    "\n",
    "* Ignoring the category 0, expenses follow two opposite patterns:\n",
    "\n",
    "  - For <span style=\"font-weight:bold;\">RoomService_Range</span>, <span style=\"font-weight:bold;\">Spa_Range</span> and <span style=\"font-weight:bold;\">VRDeck_Range</span>: the <span style=\"color:green\">higher the expense</span>, the <span style=\"color:red\">worse Transported rate</span>.\n",
    "  \n",
    "  - For <span style=\"font-weight:bold;\">FoodCourt_Range</span> and <span style=\"font-weight:bold;\">ShoppingMall_Range</span>: the <span style=\"color:green\">higher the expense</span>, the <span style=\"color:green\">better Transported rate</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"DA_FE_Name\"></a>\n",
    "### <span style=\"color:teal;font-weight:bold;\">\"Name\" feature: new feature \"Ocurrence_LastName\"</span>\n",
    "\n",
    "Is there any encoded information in the <span style=\"font-weight:bold;\">Name</span> feature? Let's see a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Name'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names come in the format \"FirstName LastName\". People bearing the same last name are likely belonging to the same family. \n",
    "\n",
    "Then, I create a new feature <span style=\"font-weight:bold;\">Ocurrence_LastName</span>, which records the ocurrence of the last name for each passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Name_Last feature (temporarily):\n",
    "if 'Name_Last' not in train_df_corr: # In case faeture it's already created\n",
    "    # Training dataset:\n",
    "    train_df_corr['Name_Last'] = train_df['Name'].apply(\n",
    "        lambda x: np.nan if x!=x else (\n",
    "            x.split(' ')[-1]))\n",
    "    # Testing dataset:\n",
    "    test_df_corr['Name_Last'] = test_df['Name'].apply(\n",
    "        lambda x: np.nan if x!=x else (\n",
    "            x.split(' ')[-1]))\n",
    "    \n",
    "# Identify ocurrences for every unique value in Name_Last:\n",
    "ocurrences = pd.concat([train_df_corr['Name_Last'], test_df_corr['Name_Last']]).value_counts().to_dict()\n",
    "for dataset in [train_df_corr,test_df_corr]:\n",
    "    dataset['Ocurrence_LastName'] = dataset['Name_Last'].apply(\n",
    "        lambda x: np.nan if x!=x else ocurrences[x])\n",
    "\n",
    "# Drop unnecessary features:\n",
    "for feature in ['Name','Name_Last']:\n",
    "    if feature in train_df_corr: # Just in case feature is already dropped\n",
    "        train_df_corr = train_df_corr.drop(feature,axis=1)\n",
    "        test_df_corr = test_df_corr.drop(feature,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of values and Transported rates:\n",
    "fig, (ax1,ax2) = plt.subplots(1, 2,figsize=(8, 3)) # Start figure\n",
    "plot_dist_hist(train_df_corr,test_df_corr,'Ocurrence_LastName',ax1,bin_step=1)\n",
    "plot_KDE(train_df_corr,'Ocurrence_LastName',ax2)     \n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1, 2,figsize=(8, 3)) # Start figure\n",
    "print('-'*10,'Outer:train, Inner:test','-'*10)\n",
    "# plot_dist_hist(train_df_corr,test_df_corr,'Ocurrence_LastName',ax1,bin_step=1)\n",
    "# plot_KDE(train_df_corr,'Ocurrence_LastName',ax2)  \n",
    "plot_dist_pie(train_df_corr,test_df_corr,'Ocurrence_LastName',ax1)\n",
    "plot_swarm(train_df_corr,'Ocurrence_LastName',ax2,xlabels_off=True)    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a few exceptions, the general behavior indicates that the more ocurrences in the last name, the worse Transported rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Feature_engineering_single_corr\"></a>\n",
    "### <span style=\"color:teal;font-weight:bold;\">Correlations II (single engineered features)</span>\n",
    "\n",
    "So far, I've modified, dropped or generated new features from single original features. Let's have a look at the correlation at the current dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nFeatures: names and data types:\\n')\n",
    "print(train_df_corr.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features are numeric, except for <span style=\"font-weight:bold;\">HomePlanet</span>, <span style=\"font-weight:bold;\">Destination</span> and <span style=\"font-weight:bold;\">Cabin_1st</span>, which have 3, 3 and 8 categories, respectively. I will encode them and then plot the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_corr_enc = encode_and_correlation(train_df_corr)\n",
    "test_df_corr_enc = encode_and_correlation(test_df_corr,plot_corr=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the first correlation matrix (original dataset), I can extract new conclusions:\n",
    "\n",
    "- <span style=\"font-weight:bold;\">Cabin_1st_A</span>, <span style=\"font-weight:bold;\">Cabin_1st_B</span> and <span style=\"font-weight:bold;\">Cabin_isP</span> are noticeably correlated with <span style=\"font-weight:bold;\">Transported</span>.\n",
    "\n",
    "- The <span style=\"font-weight:bold;\">Cabin_1st_X</span> family of features is significantly correlated to the other features, specially <span style=\"font-weight:bold;\">HomePlanet</span>.\n",
    "\n",
    "- <span style=\"font-weight:bold;\">Ocurrence_LastName</span> has moderate correlations, specially with <span style=\"font-weight:bold;\">HomePlanet</span> and some <span style=\"font-weight:bold;\">Cabin_1st_X</span>.\n",
    "\n",
    "Next, I move on to engineering new features based on the combination of two or more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Feature_engineering_combined\"></a>\n",
    "## <span style=\"color:teal;font-weight:bold;\">Combined features</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"DA_FE_FromTo\"></a>\n",
    "### <span style=\"color:teal;font-weight:bold;\">\"FromTo\" new feature (from \"HomePlanet\" and \"Destination\")</span>\n",
    "\n",
    "In the Titanic voyage, <span style=\"font-weight:bold;\">the itinerary for any passenger can be defined by their HomePlanet feature and Destination</span>. Possibilities are limited: from [Earth, Europa, Mars] to [Cancri, PSO, Trappist].\n",
    "\n",
    "<span style=\"font-weight:bold;\">I will create a new feature called FromTo, containing strings with the format \"{HomePlanet}-{Destination}\".</span> \n",
    "\n",
    "<span style=\"font-style:italic;\">Note: Any missing value in either <span style=\"font-weight:bold;\">HomePlanet</span> or <span style=\"font-weight:bold;\">Destination</span> will be translated in a missing value in <span style=\"font-weight:bold;\">FromTo</span>.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature:\n",
    "for dataset in [train_df_corr, test_df_corr]:\n",
    "    dataset[\"FromTo\"] = dataset[\"HomePlanet\"] + '-' + dataset[\"Destination\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of values and Transported rates:\n",
    "fig, (ax1,ax2) = plt.subplots(1, 2,figsize=(8, 3)) # Start figure\n",
    "print('-'*10,f' FromTo | Outer:train, Inner:test','-'*10)\n",
    "plot_dist_pie(train_df_corr,test_df_corr,\"FromTo\",ax1)\n",
    "plot_swarm(train_df_corr,\"FromTo\",ax2,xlabels_off=True)    \n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "# Display explanatory table:\n",
    "explain_stats(train_df_corr,\"FromTo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEA: Ratio (FoodCourt+ShoppingMall+1)/(RoomService+Spa+VRDeck+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Submission_results\"></a>\n",
    "# <span style=\"color:teal;font-weight:bold;\">Submission results</span>\n",
    "\n",
    "I will test this improved dataset and make a few submissions. As an internal benchmark, <span style=\"font-weight:bold;\">Ocurrence_LastName</span>the best score that I got using the original dataset was 0.79635</span>.\n",
    "\n",
    "Since some Machine Learning models don't accept NaN values, I correct the missing values with the following criteria:\n",
    "\n",
    "* Numeric features: assign mean value.\n",
    "\n",
    "* Non-numeric features: assign most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple correction for dataset:\n",
    "def simple_data_correction(df,target_col='Transported'):\n",
    "    \"\"\"\n",
    "    Corrects missing values using the average value for numeric features\n",
    "    or most frequent value for non-numeric features.\n",
    "    --- Inputs ---\n",
    "    {df} [Dataframe]: Dataframe to be corrected.\n",
    "    {target_col} [String]: Target column in the dataframe (won't be corrected)\n",
    "    --- Outputs ---\n",
    "    {df_corrected} [Dataframe]: Corrected Dataframe.\n",
    "    \"\"\"\n",
    "    df_corrected = df.copy()\n",
    "    # Determine column types:\n",
    "    col_numeric = [col for col in df.columns if \n",
    "                   df[col].dtypes != 'object' and col!=target_col]\n",
    "    col_object = [col for col in df.columns if \n",
    "                  col not in col_numeric and col!=target_col]\n",
    "    # Correct numeric columns:\n",
    "    for col in col_numeric:\n",
    "        df_corrected[col] = df[col].fillna(value=df[col].mean())\n",
    "    # Correct non-numeric columns:\n",
    "    for col in col_object:\n",
    "        most_freq = df[col].value_counts(sort=True).index[0]\n",
    "        df_corrected[col] = df[col].fillna(value=most_freq)\n",
    "    return df_corrected\n",
    "\n",
    "# Define models:\n",
    "import numpy as np\n",
    "from sklearn.ensemble import (RandomForestClassifier, \n",
    "                              GradientBoostingClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Train ML models and export the submission file:\n",
    "def train_submit_ML(model,train_df,test_df):\n",
    "    \"\"\"\n",
    "    Trains a ML model, and saves the submission file.\n",
    "    --- Inputs ---\n",
    "    {model} [Machine Learning model]: ML model to be trained.\n",
    "    {train_df} [Dataframe]: Training dataset, it must contain the\n",
    "    columns \"PassengerId\" and \"Transported\".\n",
    "    {test_df} [Dataframe]: Testing dataset, it must contain the\n",
    "    column \"PassengerId\".\n",
    "    --- Outputs ---\n",
    "    {train_score} [Float]: Training score.\n",
    "    \"\"\"\n",
    "    # Extract PassengerId and Transported columns, if applicable:\n",
    "    passID_train = train_df['PassengerId']\n",
    "    passID_test = test_df['PassengerId']\n",
    "    y_train = train_df['Transported']\n",
    "    x_train = train_df.drop(['PassengerId','Transported'],axis=1)\n",
    "    x_test = test_df.drop('PassengerId',axis=1)\n",
    "    # Initialize model with fixed seed if possible:\n",
    "    if 'random_state' in dir(model):\n",
    "        model.random_state = 0 \n",
    "    # Train the model:\n",
    "    model.fit(x_train,y_train)\n",
    "    train_score = model.score(x_train,y_train) # Get training score\n",
    "    # Make submission:\n",
    "    preds = model.predict(x_test)\n",
    "    preds = preds.astype(bool) # Ensure the values are boolean\n",
    "    df_subm = pd.DataFrame({\n",
    "        \"PassengerId\": passID_test,\n",
    "        \"Transported\": preds\n",
    "    }).to_csv('submission.csv', index=False)\n",
    "    # Final output and message:\n",
    "    print(\"Training score:\", np.round(train_score,4))\n",
    "    print(\"Submission file ready.\")\n",
    "    return train_score, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options:\n",
    "# RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "# GradientBoostingClassifier(n_estimators=100, max_depth=3)\n",
    "# XGBClassifier(n_estimators=100, max_depth=3)\n",
    "\n",
    "# Define and train simple model:\n",
    "df_subm_train = pd.concat([train_ID,simple_data_correction(train_df_corr_enc)],axis=1)\n",
    "df_subm_test = pd.concat([test_ID,simple_data_correction(test_df_corr_enc)],axis=1)\n",
    "model = XGBClassifier(n_estimators=150, max_depth=2)\n",
    "train_score, preds = train_submit_ML(model,df_subm_train,df_subm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Conclusions\"></a>\n",
    "# <span style=\"color:teal;font-weight:bold;\">Conclusions</span>\n",
    "\n",
    "Xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 3220602,
     "sourceId": 34377,
     "sourceType": "competition"
    },
    {
     "datasetId": 5501507,
     "sourceId": 9114720,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
