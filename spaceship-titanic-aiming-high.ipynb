{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Title\"></a>\n",
    "# <span style=\"color:teal;font-weight:bold;\">Spaceship Titanic üî≠ Aiming high</span>\n",
    "\n",
    "This notebook is the continuation of my first exploration <a href=\"https://www.kaggle.com/code/fertmeneses/spaceship-titanic-getting-familiar/edit/run/191353629\">Spaceship Titanic üèÅ Getting familiar</a>. There, I got a clear picture of the scoring expectation for this competition. Briefly, <span style=\"font-weight:bold;\">I'm aiming for a score above 0.81, looking for a place among the top 5% submissions</span>.\n",
    "\n",
    "In my previous work, I minimally processed the data and built simple Machine Learning models, achieving 0.79635 as the best score. This time, I <span style=\"font-weight:bold;\">plan to thoroughly analyze the data, engineer features, correct missing values, find correlations, build ML models with different complexities and optimize their hyperparameters.</span>.\n",
    "\n",
    "The methodology of this work is based on my previous notebook <a href=\"https://www.kaggle.com/code/fertmeneses/titanic-kaggle-full-analysis\">Titanic/Kaggle -Full analysis</a> from the <span style=\"font-style:italic;\">Titanic - Machine Learning from Disaster</span> competition.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='teal'>Outline</font> <a class=\"anchor\"  id=\"Outline\"></a>\n",
    "\n",
    "[**Data analysis**](#Data_analysis)\n",
    "\n",
    "  - [Load original data](#Data_analysis_load)\n",
    "  \n",
    "  - [Distribution of values and transported rates](#Data_analysis_values_rates)\n",
    "  \n",
    "  - [Correlations (original data)](#Data_analysis_correlations)\n",
    "  \n",
    "[**Data edition**](#Feature_engineering)\n",
    "\n",
    "[**Machine Learning architectures**](#ML_architectures)\n",
    "\n",
    "[**Hyperparameters optimization**](#Hyperparameters_optimization)\n",
    "  \n",
    "[**Conclusions**](#Conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data_analysis\"></a>\n",
    "# <span style=\"color:teal;font-weight:bold;\">Data analysis</span>\n",
    "\n",
    "In this section, I load the original data and analyze it without any edition. This is the workflow:\n",
    "\n",
    "- Load raw data.\n",
    "\n",
    "- Analyze distribution of values and <span style=\"font-weight:bold;\">Transported</span> rates.\n",
    "\n",
    "- Analyze correlations. <span style=\"font-style:italic;\">Note: for this step, I make a copy of the original dataset and One-Hot encode some features, but I don't use this copy for anything else.</span>\n",
    "\n",
    "<a id=\"Data_analysis_load\"></a>\n",
    "## <span style=\"color:teal;font-weight:bold;\">Load original data</span>\n",
    "\n",
    "In the following lines, I load the original datasets and get this information:\n",
    "\n",
    "- Example for 5 first rows in training dataset.\n",
    "\n",
    "- Number of rows in both datasets.\n",
    "\n",
    "- Features' names and data types.\n",
    "\n",
    "- Number of missing values in both datasets, per feature and per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Juanna Vines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Altark Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>Solam Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Willy Santantines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
       "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
       "1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False   \n",
       "2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True   \n",
       "3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False   \n",
       "4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False   \n",
       "\n",
       "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
       "0          0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
       "1        109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
       "2         43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
       "3          0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
       "4        303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
       "\n",
       "   Transported  \n",
       "0        False  \n",
       "1         True  \n",
       "2        False  \n",
       "3        False  \n",
       "4         True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows in train/test datasets:\n",
      "\n",
      "8693 / 4277\n",
      "\n",
      "Features: names and data types:\n",
      "\n",
      "PassengerId      object\n",
      "HomePlanet       object\n",
      "CryoSleep        object\n",
      "Cabin            object\n",
      "Destination      object\n",
      "Age             float64\n",
      "VIP              object\n",
      "RoomService     float64\n",
      "FoodCourt       float64\n",
      "ShoppingMall    float64\n",
      "Spa             float64\n",
      "VRDeck          float64\n",
      "Name             object\n",
      "Transported        bool\n",
      "dtype: object\n",
      "\n",
      "Missing values in train/test datasets:\n",
      "\n",
      "PassengerId:         \u001b[32m0\u001b[0m/\u001b[32m0\u001b[0m\n",
      "HomePlanet:       \u001b[31m201\u001b[0m/\u001b[31m87\u001b[0m\n",
      "CryoSleep:        \u001b[31m217\u001b[0m/\u001b[31m93\u001b[0m\n",
      "Cabin:           \u001b[31m199\u001b[0m/\u001b[31m100\u001b[0m\n",
      "Destination:      \u001b[31m182\u001b[0m/\u001b[31m92\u001b[0m\n",
      "Age:              \u001b[31m179\u001b[0m/\u001b[31m91\u001b[0m\n",
      "VIP:              \u001b[31m203\u001b[0m/\u001b[31m93\u001b[0m\n",
      "RoomService:      \u001b[31m181\u001b[0m/\u001b[31m82\u001b[0m\n",
      "FoodCourt:       \u001b[31m183\u001b[0m/\u001b[31m106\u001b[0m\n",
      "ShoppingMall:     \u001b[31m208\u001b[0m/\u001b[31m98\u001b[0m\n",
      "Spa:             \u001b[31m183\u001b[0m/\u001b[31m101\u001b[0m\n",
      "VRDeck:           \u001b[31m188\u001b[0m/\u001b[31m80\u001b[0m\n",
      "Name:             \u001b[31m200\u001b[0m/\u001b[31m94\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'N_nan_rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m N_nan_test \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Print number of rows with N missing values:\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[43mN_nan_rows\u001b[49m)\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mset\u001b[39m(N_nan_test)):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of rows with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m missing values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(N_nan_train\u001b[38;5;241m==\u001b[39mn)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(N_nan_test\u001b[38;5;241m==\u001b[39mn)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N_nan_rows' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "# Load original datasets:\n",
    "train_df = pd.read_csv('kaggle/input/spaceship-titanic/train.csv') # Training dataset\n",
    "test_df = pd.read_csv('kaggle/input/spaceship-titanic/test.csv') # Testing dataset\n",
    "display(train_df.head(5)) # Examples\n",
    "# Print global information:\n",
    "print('\\nNumber of rows in train/test datasets:\\n')\n",
    "print(len(train_df),'/',len(test_df))\n",
    "print('\\nFeatures: names and data types:\\n')\n",
    "print(train_df.dtypes)\n",
    "# Print number of missing values per feature:\n",
    "print('\\nMissing values in train/test datasets:\\n')\n",
    "for col in test_df.columns:\n",
    "    # Count missing values:\n",
    "    N_train = train_df[col].isna().sum() \n",
    "    N_test = test_df[col].isna().sum()\n",
    "    # Print results:\n",
    "    color_train = 'red' if N_train else 'green'\n",
    "    color_test = 'red' if N_test else 'green'\n",
    "    rmargin = 40-len(col)\n",
    "    print(f'{col}:',f'{colored(N_train, color_train)}/{colored(N_test, color_test)}'.rjust(rmargin))\n",
    "# Count missing values in each row:\n",
    "N_nan_train = train_df.apply(lambda x: x.isna().sum(), axis=1)\n",
    "N_nan_test = test_df.apply(lambda x: x.isna().sum(), axis=1) \n",
    "# Print number of rows with N missing values:\n",
    "for n in set(N_nan_train).union(set(N_nan_test)):\n",
    "    print(f'Number of rows with {n} missing values: {sum(N_nan_train==n)}/{sum(N_nan_test==n)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data_analysis_values_rates\"></a>\n",
    "## <span style=\"color:teal;font-weight:bold;\">Distribution of values and Transported rates</span>\n",
    "\n",
    "For a better understanding of each feature, I <span style=\"font-weight:bold;\">plot the distribution of values (column 1) in both training and testing datasets; and the Transported rates for the training dataset (column 2)</span>. Those feature with more than 10 unique values are not included in the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Define plotting functions:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_dist_pie(df_train,df_test,feature,axis):\n",
    "    \"\"\"\n",
    "    Plots a pie chart for the distribution of values from a single\n",
    "    feature in both training and testing datasets.\n",
    "    --- Inputs ---\n",
    "    {df_train, df_test} [Dataframes]: Training and testing datasets.\n",
    "    Both datasets must include the column with the input name {feature}.\n",
    "    {feature} [String]: Name of the column (feature) to be analyzed.\n",
    "    {axis} [matplotlib axis object]: Axis for the current subplot.\n",
    "    \"\"\"\n",
    "    # Identify data, count frequency per variable and sort it alphabetically:\n",
    "    train_data, test_data = df_train[feature], df_test[feature]\n",
    "    train_counts = train_data.value_counts().sort_index()\n",
    "    test_counts = test_data.value_counts().sort_index()\n",
    "    # Plot pie chart, start with the outer ring:\n",
    "    axis.pie(train_counts, colors=sns.color_palette('Set2'),\n",
    "             labels=train_counts.keys(),labeldistance=0.8,\n",
    "             startangle=90,frame=True,explode=np.ones(len(train_counts))*0.01)\n",
    "    # Add white ring to separate training and testing pies:\n",
    "    axis.add_artist(plt.Circle((0,0),0.70,color='black', fc='white',linewidth=0))\n",
    "    # Testing inner pie:\n",
    "    axis.pie(test_counts, colors=sns.color_palette('Set2'),\n",
    "             labels=None,labeldistance=0.6,\n",
    "             radius=0.5,startangle=90,explode=np.ones(len(test_counts))*0.01)\n",
    "    # Add white central circle to complete the pie:\n",
    "    axis.add_artist(plt.Circle((0,0),0.25,color='black', fc='white',linewidth=0))\n",
    "    # Set title:\n",
    "    axis.set_title(f'{feature}: Distribution')\n",
    "    \n",
    "def plot_dist_hist(df_train,df_test,feature,axis,bin_step=None):\n",
    "    \"\"\"\n",
    "    Plots a histogram chart for the distribution of values from a\n",
    "    single feature in both training and testing datasets.\n",
    "    --- Inputs ---\n",
    "    {df_train, df_test} [Dataframes]: Training and testing datasets.\n",
    "    Both datasets must include the column with the input name {feature}.\n",
    "    {feature} [String]: Name of the column (feature) to be analyzed.\n",
    "    {axis} [matplotlib axis object]: Axis for the current subplot.\n",
    "    {bin_step} [Integer or None]: If provided, set the bins' \n",
    "    step-value for the histogram, otherwise automatically assigned.\n",
    "    \"\"\"\n",
    "    # Identify relevant data and calculate fraction of valid values:\n",
    "    train_data, test_data = df_train[feature], df_test[feature]\n",
    "    train_frac = np.round(train_data.count()/len(train_data)*100,1) # [%]\n",
    "    test_frac = np.round(test_data.count()/len(test_data)*100,1) # [%]\n",
    "    # Determine binning, uniform for both datasets:\n",
    "    min_range = min(min(train_data),min(test_data))\n",
    "    max_range = max(max(train_data),max(test_data))\n",
    "    if bin_step:\n",
    "        binning = np.arange(min_range,max_range+bin_step*2,bin_step)-bin_step/2\n",
    "    else:\n",
    "        binning = np.linspace(min_range,max_range,10)\n",
    "    # Bar plot:\n",
    "    train_data.plot(kind='hist', bins=binning, edgecolor='navy', color='teal',\n",
    "                    ax=axis, alpha=0.6)\n",
    "    test_data.plot(kind='hist', bins=binning, edgecolor='navy', color='orange',\n",
    "                   ax=axis, alpha=0.6)\n",
    "    axis.legend(['Train','Test']) # Set legend   \n",
    "    axis.set_yticks([0,int(axis.get_ylim()[1])], minor=False)\n",
    "    axis.set_xlabel(feature)\n",
    "    axis.set_title(f'{feature}: Distribution')\n",
    "    for s in [\"top\",\"right\",\"left\", 'bottom']: # Remove spins\n",
    "        axis.spines[s].set_visible(False)\n",
    "\n",
    "def plot_swarm(df_train,feature,axis,ref_feature='Transported',\n",
    "                    seed=42):\n",
    "    \"\"\"\n",
    "    Plots a swarm plot ordered by the transported rate from a \n",
    "    single feature in the training dataset.\n",
    "    --- Inputs ---\n",
    "    {df_train} [Dataframe]: Training dataset, must include the\n",
    "    columns with names {feature} and {ref_feature}.\n",
    "    {feature} [String]: Name of the column (feature) to be analyzed.\n",
    "    {ref_feature} [String]: Name of the column (feature) by which {feature} will\n",
    "    be analyzed. This feature must be binary with values 0 and 1.\n",
    "    {axis} [matplotlib axis object]: Axis for the current subplot.\n",
    "    {seed} [Integer]: Seed for random scattering in swarm plots.\n",
    "    \"\"\"\n",
    "    # Identify data, count frequency per variable and sort it alphabetically:\n",
    "    train_data = df_train[feature] # Training dataset    \n",
    "    train_counts = train_data.value_counts().sort_index()\n",
    "    # For each variable, identify the transported rate and build the swarm plot:\n",
    "    np.random.seed(seed) # Random seed for swarm plots\n",
    "    for i, var in enumerate(train_counts.keys()):\n",
    "        # Determine points' location:\n",
    "        surv_rate = df_train.groupby([feature]).mean(numeric_only=True)[ref_feature].loc[var] # Transported rate\n",
    "        pp_pos = int(len(df_train[(df_train[feature]==var)])*surv_rate) # Positive transported values\n",
    "        pp_neg = int(len(df_train[(df_train[feature]==var)])*(1-surv_rate)) # Negative transported values\n",
    "        var_pos = np.random.uniform(0, surv_rate,pp_pos) # Randomly assign locations for \"positive\" points\n",
    "        var_neg = np.random.uniform(surv_rate,1,pp_neg) # Randomly assign locations for \"negative\" points              \n",
    "        # Allocate all points in plot:\n",
    "        axis.scatter(i+np.random.uniform(-0.3, 0.3, len(var_neg)), var_neg,s=10,\n",
    "                     color='#004c70', edgecolor='lightgray', alpha=0.2, label=f'{var}({ref_feature}=0)')\n",
    "        axis.scatter(i+np.random.uniform(-0.3, 0.3, len(var_pos)), var_pos,s=10,\n",
    "                     color='#004c70', edgecolor='lightgray', label=f'{var}({ref_feature}=1)')\n",
    "    # Ticks and limits:\n",
    "    axis.set_xlim(-0.5, len(train_counts)-0.5)\n",
    "    axis.set_ylim(-0.03, 1.1)\n",
    "    axis.set_xticks(np.linspace(0,len(train_counts.keys())-1,len(train_counts.keys())))\n",
    "    axis.set_xticklabels(train_counts.keys(), fontweight='bold', fontfamily='serif', fontsize=13)\n",
    "    axis.set_yticks([], minor=False)\n",
    "    axis.set_ylabel('')\n",
    "    # Spines, legend and title:\n",
    "    for s in [\"top\",\"right\",\"left\", 'bottom']:\n",
    "        axis.spines[s].set_visible(False)\n",
    "    axis.legend([0,1],title=ref_feature,loc=(0.8, 0.7), edgecolor='k')\n",
    "    axis.set_title(f'{feature}: \"{ref_feature}\" rate (Train)')\n",
    "\n",
    "import warnings # <sns.kdeplot> gives a warning I couldn't fix, I avoid displaying it...\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def plot_KDE(df_train,feature,axis,ref_feature='Transported'):\n",
    "    \"\"\"\n",
    "    Plots a kernel density estimate (KDE) plot ordered by the \n",
    "    transported rate from a single feature in the training dataset.\n",
    "    --- Inputs ---\n",
    "    {df_train} [Dataframe]: Training dataset, must include the\n",
    "    columns with names {feature} and {ref_feature}.\n",
    "    {feature} [String]: Name of the column (feature) to be analyzed.\n",
    "    {ref_feature} [String]: Name of the column (feature) by which {feature} will\n",
    "    be analyzed. This feature must be binary with values 0 and 1.\n",
    "    {axis} [matplotlib axis object]: Axis for the current subplot.\n",
    "    \"\"\"    \n",
    "    train_data = df_train[feature] # Identify relevant data\n",
    "    # Plot KDE:\n",
    "    sns.kdeplot(x=feature, data=df_train, ax=axis, fill=True,cut=0,\n",
    "                bw_method=0.15, lw=1.4, edgecolor='lightgray', hue=ref_feature,\n",
    "                multiple=\"stack\", palette='PuBu', alpha=0.8)\n",
    "    axis.set_yticks([], minor=False)\n",
    "    axis.set_ylabel('Density [arb. units]')\n",
    "    axis.set_title(f'{feature}: \"{ref_feature}\" rate (Train)')\n",
    "    for s in [\"top\",\"right\",\"left\", 'bottom']: # Remove spins\n",
    "        axis.spines[s].set_visible(False)  \n",
    "\n",
    "def explain_stats(df_train,feature,unique_vals_max=10):\n",
    "    \"\"\"\n",
    "    Explains the distribution of values in a table format and the\n",
    "    transported rates for the training dataset.\n",
    "    --- Inputs ---\n",
    "    {df_train} [Dataframe]: Training dataset, must include the\n",
    "    columns with names {feature} and {ref_feature}.\n",
    "    {feature} [String] Name of the column (feature) to be analyzed.\n",
    "    {unique_vals_max} [Integer]: Maximum number of unique values \n",
    "    for the feature. If there are more than this limit, there \n",
    "    won't be any output table.\n",
    "    \"\"\"\n",
    "    # Check the unique_vals_max condition (avoid NaN):\n",
    "    unique_vals = {x for x in df_train[feature] if x == x} # Number of unique values\n",
    "    if len(unique_vals) > unique_vals_max:\n",
    "        print(f'There are {len(unique_vals)} unique values for this feature, more than the allowed limit ({unique_vals_max}).')\n",
    "        return None    \n",
    "    # Explained values:\n",
    "    df_expl = pd.DataFrame(columns=[f'{feature}','#Passengers','Transported_Rate[%]'])\n",
    "    df_expl[f'{feature}'] = sorted(unique_vals)\n",
    "    df_expl['#Passengers'] = [len(df_train[df_train[f'{feature}'] == x])\n",
    "                              for x in sorted(unique_vals)]\n",
    "    # Transported rates for each feature category:\n",
    "    df_expl['Transported_Rate[%]'] = [\n",
    "        len(df_train[(df_train[feature] == x) & # Select fare range\n",
    "            (df_train['Transported'] == 1)])/ # Passengers who were transported\n",
    "        len(df_train[(df_train[feature] == x)]) # Total passengers\n",
    "        for x in sorted(unique_vals) # Iterate through all feature categories\n",
    "    ]\n",
    "    df_expl['Transported_Rate[%]'] *=100 # Convert from fraction to [%]\n",
    "    print(f'Explained {feature} values and transported rates in training dataset:')\n",
    "    display(df_expl.style.hide())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features to analyze:\n",
    "features = [feat for feat in test_df.columns if \n",
    "            test_df[feat].dtype != 'object' or # Numeric or...\n",
    "            test_df[feat].nunique()<10] # ...Few unique values\n",
    "\n",
    "# Plot distribution of values and transported rates for each feature:\n",
    "for feature in features:\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2,figsize=(8, 3)) # Start figure\n",
    "    if train_df[feature].dtype == 'object': # Non-numerical features:\n",
    "        print('-'*10,f'{feature} | Outer:train, Inner:test','-'*10)\n",
    "        plot_dist_pie(train_df,test_df,feature,ax1)\n",
    "        plot_swarm(train_df,feature,ax2)    \n",
    "    else: # Numerical features:\n",
    "        print('-'*25,feature,'-'*25)\n",
    "        plot_dist_hist(train_df,test_df,feature,ax1)\n",
    "        plot_KDE(train_df,feature,ax2)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    # Display explanatory table only if there are equal or less than 10 unique values:\n",
    "    if len(set(train_df[feature]))<=10:\n",
    "        explain_stats(train_df,feature)\n",
    "    else:\n",
    "        print('(Not suitable for a table)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary about the distribution of values and transported rates:\n",
    "\n",
    "| Feature | <font color=blue> Distribution </font> | <font color=green> Transported rates </font> |\n",
    "| :---: | :--- | :--- |\n",
    "| <span style=\"font-weight:bold;\">HomePlanet</span> | <font color=blue> Half of the passengers come from Earth, 1/4 from Europa and 1/4 from Mars  </font> | <font color=green> Europa has a slightly good rate (65%), Mars average (50%) and Earth slightly poor (40%) </font> |\n",
    "| <span style=\"font-weight:bold;\">CryoSleep</span> | <font color=blue> Only 1/3 of passengers were in CryoSleep  </font> | <font color=green> Excellent chances (80%) if Cryosleep, poor (30%) if not </font> |\n",
    "| <span style=\"font-weight:bold;\">Destination</span> | <font color=blue> 2/3 of passengers were going to Trappist, 1/4 to Cancri and 10% to PSO </font> | <font color=green> Those going to Cancri have good chances (60%), the rest average (50%) </font> |\n",
    "| <span style=\"font-weight:bold;\">Age</span> | <font color=blue> From 0 to 80, peak around 20-30 </font> | <font color=green> Good chances for the very young (less than 10 years old?), maybe average for the rest </font> |\n",
    "| <span style=\"font-weight:bold;\">VIP</span> | <font color=blue> Only 200 passengers are VIP </font> | <font color=green> VIP passengers have slightly bad chances (40%) </font> |\n",
    "| <span style=\"font-weight:bold;\">RoomService, FoodCourt, ShoppingMall, Spa, VRDeck</span> | <font color=blue> Only a few passengers spend more than a little money in services </font> | <font color=green> Can't tell from the plots </font> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data_analysis_correlations\"></a>\n",
    "## <span style=\"color:teal;font-weight:bold;\">Correlations (original data)</span>\n",
    "\n",
    "Next, I study the <span style=\"font-weight:bold;\">correlations in the original data, using the training dataset, only for those features analyzed in the previous section</span>. I build a correlation matrix with a 0.1 threshold correlation value, then only high correlations are painted.\n",
    "\n",
    "Note: the correlation matrix needs numerical or boolean features. For this purpose, I One-Hot encode the non-numerical features, and group family-encoded-features in dotted traingles within the correlation matrix (correlations there are irrelevant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# One-Hot encode non-numerical features with few unique values:\n",
    "feat_enc = [feat for feat in train_df.columns if \n",
    "            train_df[feat].dtype == 'object' and # Numeric or...\n",
    "            train_df[feat].nunique()<10] # ...Few unique values\n",
    "\n",
    "# Encode features:\n",
    "data_train_enc = pd.get_dummies(train_df[feat_enc])\n",
    "# Prepare dataframes having only numeric/boolean features:\n",
    "train_df_enc = pd.concat([train_df, data_train_enc], axis=1)\n",
    "# Sort columns alphabetically, but leave 'Transported' at the beggining in training dataset:\n",
    "train_df_enc = train_df_enc.reindex(sorted(train_df_enc.columns), axis=1)\n",
    "train_df_enc = train_df_enc[['Transported'] + [col for col in train_df_enc.columns\n",
    "                                               if col != 'Transported' ]]\n",
    "# Drop source-encoded features: \n",
    "train_df_enc = train_df_enc.drop(feat_enc,axis=1)\n",
    "# Drop not suitable features:\n",
    "feat_not_enc = [feat for feat in train_df.columns if\n",
    "                train_df[feat].dtype == 'object' and # Numeric or...\n",
    "                train_df[feat].nunique()>=10] # ...Few unique values\n",
    "train_df_enc = train_df_enc.drop(feat_not_enc,axis=1)\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Prepare data:\n",
    "corr_train = train_df_enc.corr() # Obtain correlations\n",
    "trimask = np.triu(np.ones_like(corr_train, dtype=bool)) # Mask upper triangle in correlations\n",
    "c_thres = 0.1 # Threshold value to show correlations\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "# Plot correlations in training dataset:\n",
    "sns.heatmap(corr_train, ax=ax, square=True, \n",
    "            cmap='coolwarm', vmin=-1, vmax=1,\n",
    "            linecolor='w',lw=0.5, \n",
    "            mask=trimask | (np.abs(corr_train) <= c_thres))\n",
    "# Separate the \"Survived\" correlations:\n",
    "ax.plot([1,1],[1,len(train_df_enc.columns)],color='k',lw=2)\n",
    "# Draw the correlations' triangle:\n",
    "ax.plot([1,len(train_df_enc.columns)],[1,len(train_df_enc.columns)],color='k',lw=0.5)\n",
    "ax.plot([1,len(train_df_enc.columns)],[len(train_df_enc.columns),len(train_df_enc.columns)],\n",
    "        color='k',lw=1.5)\n",
    "# Identify blocks of similar features:\n",
    "main_name = [feat.split('_')[0] for feat in list(train_df_enc.columns)]\n",
    "index_counts = [(main_name.index(name),main_name.count(name))\n",
    "                for name in sorted(set(main_name))]\n",
    "for index, counts in index_counts:\n",
    "    if counts>1: \n",
    "        triangle = [[index, index],\n",
    "                    [index+counts, index+counts],\n",
    "                    [index, index+counts]]\n",
    "        ax.add_patch(patches.Polygon(triangle, edgecolor=\"k\",facecolor='none',ls=\"--\",lw=2))\n",
    "ax.set_title(f\"Training Dataset Correlations\", size=15)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T08:54:31.420188Z",
     "iopub.status.busy": "2024-08-07T08:54:31.419691Z",
     "iopub.status.idle": "2024-08-07T08:54:32.254539Z",
     "shell.execute_reply": "2024-08-07T08:54:32.253400Z",
     "shell.execute_reply.started": "2024-08-07T08:54:31.420151Z"
    }
   },
   "source": [
    "I'm going to do a more detailed analysis later, but from this plot I highlight the following facts:\n",
    "\n",
    "<span style=\"font-weight:bold;\">Chances of transportation (first column):</span> \n",
    "- <span style=\"font-weight:bold;\">CryoSleep</span>: Cryo-sleeping (True) is better.\n",
    "- <span style=\"font-weight:bold;\">Destination</span>: going to Cancri is better.\n",
    "- <span style=\"font-weight:bold;\">HomePlanet</span>: being from Earth is worse, from Europa is better. \n",
    "- <span style=\"font-weight:bold;\">RoomService</span>: paying more is worse.\n",
    "- <span style=\"font-weight:bold;\">Spa</span>: paying more is worse.\n",
    "- <span style=\"font-weight:bold;\">VRDeck</span>: paying more is worse.\n",
    "\n",
    "<span style=\"font-style:italic;\">Note: the correlation matrix is strongly affected by the number of values with significant correlations, that's why the <span style=\"font-weight:bold;\">VIP</span> feature is not painted, even though being VIP is an important indicator for Transportation. </span>\n",
    "\n",
    "<span style=\"font-weight:bold;\">Other observations (main triangle):</span>\n",
    "\n",
    "- Cryo-sleeping passengers spend less in services \n",
    "(<span style=\"font-weight:bold;\">FoodCourt</span>\n",
    "<span style=\"font-weight:bold;\">RoomService</span>, \n",
    "<span style=\"font-weight:bold;\">ShoppingMall</span>,\n",
    "<span style=\"font-weight:bold;\">Spa</span>,\n",
    "<span style=\"font-weight:bold;\">VRDeck</span>).\n",
    "- The older the passenger, the more more likely is that they are from Europa. On the contrary, younger passengers are more likely to come from Earth.\n",
    "- The following origin-destinations are more probable: Europa-Cancri, Earth-PSO, Mars-Trappist.\n",
    "- Passengers from Mars are more likely to spend more in RoomService and ShoppingMall.\n",
    "- Passengers from Europa are more likely to spend more in Spa and VRDeck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data_edition\"></a>\n",
    "# <span style=\"color:teal;font-weight:bold;\">Data edition</span>\n",
    "\n",
    "Correct missing values + Featuring engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data_analysis_correlations_HomePlanet\"></a>\n",
    "### <span style=\"color:teal;font-weight:bold;\">HomePlanet</span>\n",
    "\n",
    "<span style=\"font-weight:bold;\">In this feature and in all the ones following, I choose a different feature to make a plot and group the results according to the Transportation status.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_features(df,feat_A,feat_B):\n",
    "    \"\"\"\n",
    "    XXX\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 6)) # Start figure\n",
    "    ax.scatter(df[feat_A], df[feat_B])\n",
    "    #ax.plot(df[feat_A],df[feat_B])\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HomePlanet vs. Destination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.copy()\n",
    "feat_A = 'HomePlanet'\n",
    "feat_B = 'Destination'\n",
    "values_A = [x for x in set(df[feat_A]) if x==x]\n",
    "values_B = [x for x in set(df[feat_B]) if x==x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[feat_A]+'-'+df[feat_B]).value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_two_features(train_df,'HomePlanet','Destination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ML_architectures\"></a>\n",
    "# <span style=\"color:teal;font-weight:bold;\">Machine Learning architectures</span>\n",
    "\n",
    "Xxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Hyperparameters_optimization\"></a>\n",
    "# <span style=\"color:teal;font-weight:bold;\">Hyperparameters optimization</span>\n",
    "\n",
    "Xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Conclusions\"></a>\n",
    "# <span style=\"color:teal;font-weight:bold;\">Conclusions</span>\n",
    "\n",
    "Xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 3220602,
     "sourceId": 34377,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
